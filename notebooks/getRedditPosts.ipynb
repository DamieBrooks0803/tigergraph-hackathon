{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8756d4f2-6cb1-42a1-87f6-c11ff2904a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import sched, time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "s = sched.scheduler(time.time, time.sleep)\n",
    "subreddit = 'all'\n",
    "limit = 100\n",
    "timeframe = 'month'  # hour, day, week, month, year, all\n",
    "listing = 'controversial'  # controversial, best, hot, new, random, rising, top\n",
    "filename = datetime.now().strftime('new-%Y-%m-%d-%H-%M.csv')\n",
    "after = \"\"\n",
    "sample_str = \"Test&[88]%%$$$#$%-+String\"\n",
    "\n",
    "\n",
    "def get_reddit_JSON(subreddit, listing, limit, timeframe):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/{listing}.json?limit={limit}&t={timeframe}'\n",
    "        request = requests.get(base_url, headers={'User-agent': 'yourbot'})\n",
    "\n",
    "    except:\n",
    "        print('An Error Occurred in pulling from Reddit')\n",
    "\n",
    "    return request.json()\n",
    "\n",
    "def get_reddit_JSONNext(subreddit, listing, limit, timeframe,after):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/{listing}.json?limit={limit}&t={timeframe}&after={after}'\n",
    "        request = requests.get(base_url, headers={'User-agent': 'yourbot'})\n",
    "\n",
    "    except:\n",
    "        print('An Error Occurred in pulling from Reddit')\n",
    "\n",
    "    return request.json()\n",
    "\n",
    "\n",
    "def get_results(r):\n",
    "    myDict = {}\n",
    "    for post in r['data']['children']:\n",
    "        myDict[post['data']['title']] = {\n",
    "            'listingID': removeSpaces(post['data']['title']) + \"_\" + post['data']['subreddit'],\n",
    "            'createdDate': post['data']['created_utc'],\n",
    "            'redditCallType': listing,\n",
    "            'url': post['data']['permalink'],\n",
    "            'score': post['data']['score'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'title': cleanChars(post['data']['title']),\n",
    "            'sentimentTitle': sentimentAnalyze(post['data']['title']),\n",
    "            'sentimentSCore': sentimentScorer(post['data']['title']),\n",
    "            'sentimentBody': sentimentAnalyze(post['data']['selftext']),\n",
    "            'comments': post['data']['num_comments'],\n",
    "            'author': post['data']['author'],\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'body': cleanChars(post['data']['selftext'])\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(myDict, orient='index')\n",
    "    #print(\"Reddit scraped\")\n",
    "    return df\n",
    "\n",
    "def removeSpaces(sample_str2):\n",
    "    pattern = r'[^A-Za-z0-9]+'\n",
    "    titleNoSpaces = re.sub(pattern, '', sample_str2)\n",
    "    if titleNoSpaces is None:\n",
    "        titleNoSpaces = \"   \"\n",
    "    return titleNoSpaces\n",
    "\n",
    "def cleanChars(sample_str):\n",
    "\n",
    "    # Create a regex pattern to match all special characters in string\n",
    "    pattern = r'[^A-Za-z0-9 ]+'\n",
    "    # Remove special characters from the string\n",
    "    newString = re.sub(pattern, ' ', sample_str)\n",
    "    return newString\n",
    "\n",
    "def sentimentAnalyze(sample_str3):\n",
    "    #sentence = \"I am very sad today.\"\n",
    "    sentence = sample_str3\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif sentiment_dict['compound'] <= - 0.05:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    return sentiment\n",
    "\n",
    "def sentimentScorer(sample_str3):\n",
    "    #sentence = \"I am very sad today.\"\n",
    "    sentence = sample_str3\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    return sentiment_dict['compound']\n",
    "\n",
    "\n",
    "r = get_reddit_JSON(subreddit, listing, limit, timeframe)\n",
    "f = get_results(r)\n",
    "z = f.copy()\n",
    "after = r['data']['after']\n",
    "numCalls = 0\n",
    "while numCalls < 301:\n",
    "#while numCalls < 4000:\n",
    "    k = get_reddit_JSONNext(subreddit, listing, limit, timeframe, after); n = get_results(k); z = pd.concat([z, n]); after = k['data']['after']; numCalls +=1\n",
    "filename = datetime.now().strftime('con_Month_%Y-%m-%d-%H-%M.csv')\n",
    "z.to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1631d-da10-47f3-8f1e-bf8258edbe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
